{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distutils.log import error\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "# np_config.enable_numpy_behavior()\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Networks\n",
    "from networks import *\n",
    "\n",
    "# Helpers\n",
    "from helpers import *\n",
    "\n",
    "# Main Coop_MADDPG class\n",
    "from MADDPG import *\n",
    "\n",
    "# Rando and test functions\n",
    "from rando_test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, continuous, total_trials=1, total_episodes=100, \n",
    "            disc_actions_num=2, seed=1453,\n",
    "            buffer_capacity=50000, batch_size=64, num_agents=4, learn_step=25,\n",
    "            std_dev=0.3, epsilon=0.2, actor_lr=0.002, critic_lr=0.003, clip=1,\n",
    "            gamma=0.99, tau=0.005, adam_eps=1e-07, amsgrad=False, theta=0.15,\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed, std_dev_func=fixed, epsilon_func=fixed,\n",
    "            reward_mod=False, start_steps=0, loss_func=losses.MeanAbsoluteError(),\n",
    "            mean_number=20, solved=999,\n",
    "            render=False, weights_directory='Weights/', plots_directory='Graphs/', \n",
    "            output=True, total_time=True, use_gpu=True, return_data=False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # _ = env.seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    num_states = env.observation_space[0].shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "    \n",
    "    if not use_gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    # env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Coop_MADDPG(num_states, num_actions, num_agents, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, epsilon,\n",
    "            actor_lr, critic_lr, gamma, tau, clip,\n",
    "            adam_eps, amsgrad, theta, disc_actions_num, loss_func)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            before = time.time()\n",
    "            \n",
    "            agent.gamma = gamma_func(agent.gamma, ep)\n",
    "            agent.tau = tau_func(agent.tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(agent.critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(agent.actor_lr, ep)\n",
    "            agent.std_dev = std_dev_func(agent.std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(agent.epsilon, ep)\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = np.zeros(num_agents)\n",
    "            true_reward = np.zeros(num_agents)\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.convert_to_tensor(prev_state)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, rng=rng)\n",
    "                else:\n",
    "                    action = two_mini_random(num_agents)\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                # TESTING **********************************\n",
    "                # time.sleep(0.5)\n",
    "                # print(action)\n",
    "                \n",
    "                if continuous:\n",
    "                    # try:\n",
    "                    #     len(action)\n",
    "                    # except:\n",
    "                    #     action = [action]\n",
    "                    # state, reward, done, info = env.step(action)\n",
    "                    raise ValueError('not implemented continuous')\n",
    "                else:\n",
    "                    # take the argmax to get correct action format [0, 1] instead of [(0.7,0.3), (0.2,0.8)]\n",
    "                    state, reward, done, info = env.step(np.argmax(action, axis=1).tolist())\n",
    "                \n",
    "                true_reward = true_reward.__add__(reward)\n",
    "                \n",
    "                terminal_state = np.array(np.invert(done),dtype=np.float32)\n",
    "\n",
    "                # Reward modification\n",
    "                if reward_mod:\n",
    "                    # invert reward\n",
    "                    # reward = [-r for r in reward]\n",
    "\n",
    "                    if all(done):\n",
    "                        reward_addition = [10]*num_agents\n",
    "                        reward = [reward[i] + reward_addition[i] for i in range(len(reward))]\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "\n",
    "                if step % learn_step == 0:\n",
    "                    agent.learn()\n",
    "                    update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                    update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                episodic_reward = episodic_reward.__add__(reward)\n",
    "\n",
    "                prev_state = state\n",
    "                \n",
    "                if all(done):\n",
    "                    break\n",
    "\n",
    "            list_reward = episodic_reward\n",
    "            episodic_reward = np.sum(episodic_reward)\n",
    "            true_reward = np.sum(true_reward)\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                if reward_mod:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "                else:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * ListRew {} * Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, str(list_reward), episodic_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        save_weights(agent, env, weights_directory, trial)\n",
    "    \n",
    "    # Plotting graph\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    save_name = \"{}_{}_{}\".format(env.spec.id, agent.continuous, timestamp)\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig(plots_directory + save_name + '.png')\n",
    "    except:\n",
    "        print('Graph save fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - start_time, 's')\n",
    "    \n",
    "    if return_data:\n",
    "        return agent, ep_reward_list, true_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ma_gym:TrafficJunction4-v0')\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "# print(env.observation_space)\n",
    "obs, rew, done, _ = env.step(np.random.randint(0,2,(4)).tolist())\n",
    "print(obs[0].shape)\n",
    "print(rew)\n",
    "print(done)\n",
    "env.reset()\n",
    "env._max_steps = 80 # reduce from 100 for faster environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f78c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, rl, trl = run(env, False, total_trials=1, total_episodes=3000, \n",
    "        disc_actions_num=2, seed=1453,\n",
    "        buffer_capacity=100000, batch_size=64, num_agents=4, learn_step=2,\n",
    "        std_dev=0.3, epsilon=0.1, actor_lr=0.001, critic_lr=0.002, clip=3,\n",
    "        gamma=0.99, tau=0.002, adam_eps=1e-07, amsgrad=False, theta=0.15,\n",
    "        gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed, std_dev_func=fixed, epsilon_func=fixed,\n",
    "        reward_mod=True, start_steps=500, loss_func=losses.MeanSquaredError(),\n",
    "        mean_number=40, solved=-6,\n",
    "        render=False, weights_directory='Weights/', plots_directory='Graphs/', \n",
    "        output=True, total_time=True, use_gpu=True, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baed6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_actor(81, 1, False, 2)\n",
    "b = get_critic(81, 4, 1, False, 2)\n",
    "obs = np.array(obs)\n",
    "state = np.stack((obs, obs)*32)\n",
    "print('state shape',state.shape)\n",
    "rewo = np.expand_dims(np.stack((rew, rew)*32),-1)\n",
    "print('rewo shape', rewo.shape)\n",
    "dd = np.array(np.invert(done),dtype=np.float32)\n",
    "ddd = np.expand_dims(np.stack((dd,dd)*32),-1)\n",
    "print('done shape', ddd.shape)\n",
    "\n",
    "state_first_flat = state.reshape(-1, state.shape[-1])\n",
    "print(state_first_flat.shape)\n",
    "state_last_flat = state.reshape(state.shape[0], -1)\n",
    "print(state_last_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6731697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rando(env, continuous=False, total_episodes=10, render=True, timing=False, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca. best case reward\n",
    "-np.sum(np.arange(0, 0.01*14, 0.01)) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = rando(env, total_episodes=1000, render=False, timing=False, testing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rel)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c878ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rl[0])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72597640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(rl[0]))\n",
    "print(np.mean(rel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.done_buffer.tolist()\n",
    "with open(\"debug/done_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6031cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.action_buffer.tolist()\n",
    "with open(\"debug/action_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc527860",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.state_buffer.tolist()\n",
    "with open(\"debug/state_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.next_state_buffer.tolist()\n",
    "with open(\"debug/next_state_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfaa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.reward_buffer.tolist()\n",
    "with open(\"debug/reward_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_world()\n",
    "# reset_world()\n",
    "# reward()\n",
    "# observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('simple.py')\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo.butterfly import pistonball_v6\n",
    "# parallel_env = pistonball_v6.parallel_env()\n",
    "# observations = parallel_env.reset()\n",
    "# env = pistonball_v6.parallel_env()\n",
    "# parallel_api_test(env, num_cycles=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
