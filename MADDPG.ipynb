{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distutils.log import error\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "def get_actor(num_states, num_actions, continuous, disc_actions_num):\n",
    "    \n",
    "    ### ACTOR NETWORK ###\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(128, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    \n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=initializers.RandomNormal(stddev=0.03))(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=initializers.RandomNormal(stddev=0.03))(out)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num):\n",
    "    \n",
    "    ### CRITIC NETWORK ###\n",
    "    \n",
    "    state_input = layers.Input(shape=(num_states * num_agents))\n",
    "    state_out = layers.Dense(64, activation=\"relu\")(state_input)\n",
    "    \n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions * num_agents))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num * num_agents))\n",
    "    action_out = layers.Dense(64, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(128, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    return Model([state_input, action_input], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8df9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "        \n",
    "def fixed(x, episode):\n",
    "    return x\n",
    "\n",
    "def save_weights(agent, env, weights_directory, trial):\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    save_name = \"{}_{}_{}\".format(env.spec.id, agent.continuous, timestamp)\n",
    "    try:\n",
    "        agent.actor_model.save_weights(weights_directory + 'actor-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "    except:\n",
    "        print('actor save fail')\n",
    "    try:\n",
    "        agent.critic_model.save_weights(weights_directory + 'critic-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "    except:\n",
    "        print('critic save fail')\n",
    "\n",
    "def two_mini_random(num_agents):\n",
    "    out = []\n",
    "    for a in range(num_agents):\n",
    "        bat = np.random.uniform(0,1,2)\n",
    "        bat[1] = 1 - bat[0]\n",
    "        out.append(bat.tolist())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coop_MADDPG:\n",
    "    def __init__(self, num_states, num_actions, num_agents, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, epsilon,\n",
    "            actor_lr, critic_lr, gamma, tau, min_clip, max_clip,\n",
    "            adam_eps, amsgrad, theta, disc_actions_num, loss_func):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_agents, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_agents, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_agents, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, num_agents, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_agents, num_states))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, num_agents, 1), np.float32)\n",
    "        self.std_dev = std_dev # For continuous\n",
    "        self.epsilon = epsilon # Epsilon greedy for discrete\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.disc_actions_num = disc_actions_num\n",
    "        self.num_agents = num_agents\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        self.min_clip = min_clip\n",
    "        self.max_clip = max_clip\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1), theta=theta)\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, continuous, disc_actions_num)\n",
    "        self.critic_model = get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num)\n",
    "        self.target_actor = get_actor(num_states, num_actions, continuous, disc_actions_num)\n",
    "        self.target_critic = get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num)\n",
    "        \n",
    "        self.actor_optimizer = optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_eps, amsgrad=amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_eps, amsgrad=amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "    \n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        for agent in range(self.num_agents):\n",
    "            self.state_buffer[index][agent] = obs_tuple[0][agent]\n",
    "            self.action_buffer[index][agent] = obs_tuple[1][agent]\n",
    "            self.reward_buffer[index][agent] = obs_tuple[2][agent]\n",
    "            self.next_state_buffer[index][agent] = obs_tuple[3][agent]\n",
    "            self.done_buffer[index][agent] = obs_tuple[4][agent]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, loss_func):\n",
    "\n",
    "        # state_batch = state_batch.reshape(*state_batch.shape[:1], -1)\n",
    "        # action_batch = action_batch.reshape(*action_batch.shape[:1], -1)\n",
    "        # reward_batch = reward_batch.reshape(*reward_batch.shape[:1], -1)\n",
    "        # next_state_batch = next_state_batch.reshape(*next_state_batch.shape[:1], -1)\n",
    "        # done_batch = done_batch.reshape(*done_batch.shape[:1], -1)\n",
    "        #print(next_state_batch[:][0])\n",
    "        flat_next_state = next_state_batch.reshape(*next_state_batch.shape[:1], -1)\n",
    "        flat_state = state_batch.reshape(*state_batch.shape[:1], -1)\n",
    "        flat_action = action_batch.reshape(*action_batch.shape[:1], -1)\n",
    "\n",
    "        # calculate per agent loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = []\n",
    "            target_actions = np.zeros((self.batch_size, self.num_agents, self.disc_actions_num))\n",
    "            for agent in range(self.num_agents):\n",
    "                target_actions[:,agent,:] = self.target_actor(next_state_batch[:,agent,:], training=True).numpy()\n",
    "            target_actions = tf.convert_to_tensor(target_actions)\n",
    "            target_actions = target_actions.reshape(*target_actions.shape[:1], -1)\n",
    "            for agent in range(self.num_agents):\n",
    "                y.append(reward_batch[:,agent,:] + done_batch[:,agent,:] * self.gamma * self.target_critic([flat_next_state, target_actions], training=True))\n",
    "            critic_value = self.critic_model([flat_state, flat_action], training=True)\n",
    "            critic_loss = loss_func(sum(y), critic_value)\n",
    "\n",
    "        # get the gradient for the critic\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=self.min_clip, clip_value_max=self.max_clip), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        # calculate per agent\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = np.zeros((self.batch_size, self.num_agents, self.disc_actions_num))\n",
    "            for agent in range(self.num_agents):\n",
    "                actions[:,agent,:] = self.actor_model(state_batch[:,agent,:], training=True).numpy()\n",
    "            actions = tf.convert_to_tensor(target_actions)\n",
    "            actions = actions.reshape(*actions.shape[:1], -1)\n",
    "            critic_value = self.critic_model([flat_state, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        # get the gradient for the critic\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "\n",
    "        # Gradient clipping\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        *avgd, = actor_gvd\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=self.min_clip, clip_value_max=self.max_clip), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, self.loss_func)\n",
    "\n",
    "    def policy(self, state, noise_object=0, use_noise=True, rng=np.random.default_rng()):\n",
    "        if use_noise:\n",
    "            if self.continuous:\n",
    "                # sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                # noise = noise_object()\n",
    "                \n",
    "                # sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "                # # We make sure action is within bounds\n",
    "                # legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                # return [np.squeeze(legal_action)][0]\n",
    "                raise ValueError('not implemented continuous')\n",
    "            else:\n",
    "                if (np.random.uniform(0,1,1)[0] < self.epsilon): #not currently using seed\n",
    "                    # Return random array of actions (can be above sum of 1, but should not matter that much?)\n",
    "                    actions = two_mini_random(self.num_agents)\n",
    "                    return actions\n",
    "                else:\n",
    "                    # Return output of the actor network for each of the observed states\n",
    "                    actions = []\n",
    "                    for tf_obs in state[0]:\n",
    "                        actions.append(self.actor_model(tf.expand_dims(tf_obs,0)).tolist())\n",
    "                    return np.array(actions).squeeze().tolist()\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                # sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                # legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                # return [np.squeeze(legal_action)][0]\n",
    "                raise ValueError('not implemented continuous')\n",
    "            else:\n",
    "                actions = []\n",
    "                for tf_obs in state[0]:\n",
    "                    actions.append(self.actor_model(tf.expand_dims(tf_obs,0)).tolist())\n",
    "                return np.array(actions).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aaffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ac1 = get_actor(81, 1, False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d189a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = np.zeros(81)\n",
    "# olong = [o]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = []\n",
    "# for tf_obs in olong:\n",
    "#     actions.append(ac1(tf.expand_dims(tf_obs,0)).tolist())\n",
    "# np.array(actions).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, continuous, total_trials=1, total_episodes=100, \n",
    "            disc_actions_num=2, seed=1453,\n",
    "            buffer_capacity=50000, batch_size=64, num_agents=4, learn_step=25,\n",
    "            std_dev=0.3, epsilon=0.2, actor_lr=0.002, critic_lr=0.003, min_clip=-1, max_clip=1,\n",
    "            gamma=0.99, tau=0.005, adam_eps=1e-07, amsgrad=False, theta=0.15,\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed, std_dev_func=fixed, epsilon_func=fixed,\n",
    "            reward_mod=False, start_steps=0, loss_func=losses.MeanAbsoluteError(),\n",
    "            mean_number=20, solved=999,\n",
    "            render=False, weights_directory='Weights/', plots_directory='Graphs/', \n",
    "            output=True, total_time=True, use_gpu=True, return_data=False):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # _ = env.seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    num_states = env.observation_space[0].shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "    \n",
    "    if not use_gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    # env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Coop_MADDPG(num_states, num_actions, num_agents, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, epsilon,\n",
    "            actor_lr, critic_lr, gamma, tau, min_clip, max_clip,\n",
    "            adam_eps, amsgrad, theta, disc_actions_num, loss_func)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            before = time.time()\n",
    "            \n",
    "            agent.gamma = gamma_func(agent.gamma, ep)\n",
    "            agent.tau = tau_func(agent.tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(agent.critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(agent.actor_lr, ep)\n",
    "            agent.std_dev = std_dev_func(agent.std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(agent.epsilon, ep)\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = np.zeros(num_agents)\n",
    "            true_reward = np.zeros(num_agents)\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, rng=rng)\n",
    "                else:\n",
    "                    action = two_mini_random(num_agents)\n",
    "                \n",
    "                step += 1\n",
    "\n",
    "                # TESTING **********************************\n",
    "                # time.sleep(0.5)\n",
    "                # print(action)\n",
    "                \n",
    "                if continuous:\n",
    "                    # try:\n",
    "                    #     len(action)\n",
    "                    # except:\n",
    "                    #     action = [action]\n",
    "                    # state, reward, done, info = env.step(action)\n",
    "                    raise ValueError('not implemented continuous')\n",
    "                else:\n",
    "                    # take the argmax to get env format\n",
    "                    output_action = []\n",
    "                    for act in action:\n",
    "                        output_action.append(np.argmax(act))\n",
    "                    state, reward, done, info = env.step(output_action)\n",
    "                \n",
    "                true_reward = true_reward.__add__(reward)\n",
    "                \n",
    "                terminal_state = np.array(np.invert(done),dtype=np.float32)\n",
    "\n",
    "                # Reward modification\n",
    "                if reward_mod:\n",
    "                    if all(done):\n",
    "                        reward_addition = [10]*num_agents\n",
    "                        reward = [reward[i] + reward_addition[i] for i in range(len(reward))]\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "\n",
    "                if step % learn_step == 0:\n",
    "                    agent.learn()\n",
    "                    update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                    update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                episodic_reward = episodic_reward.__add__(reward)\n",
    "\n",
    "                prev_state = state\n",
    "                \n",
    "                if all(done):\n",
    "                    break\n",
    "\n",
    "            list_reward = episodic_reward\n",
    "            episodic_reward = np.sum(episodic_reward)\n",
    "            true_reward = np.sum(true_reward)\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                if reward_mod:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, true_avg_reward, episodic_reward, true_reward, (time.time() - before), step))\n",
    "                else:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * ListRew {} * Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, str(list_reward), episodic_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        save_weights(agent, env, weights_directory, trial)\n",
    "    \n",
    "    # Plotting graph\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    save_name = \"{}_{}_{}\".format(env.spec.id, agent.continuous, timestamp)\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig(plots_directory + save_name + '.png')\n",
    "    except:\n",
    "        print('Graph save fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - start_time, 's')\n",
    "    \n",
    "    if return_data:\n",
    "        return agent, ep_reward_list, true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, actor_weights, continuous, total_episodes=10, render=False, timing=False):\n",
    "    rewards = []\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Coop_MADDPG()\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "\n",
    "            if continuous:\n",
    "                # try:\n",
    "                #     len(action)\n",
    "                # except:\n",
    "                #     action = [action]\n",
    "                state, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                state, reward, done, info = env.step(np.argmax(action))\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        if timing:\n",
    "            print(str(time.time() - before) + 's')\n",
    "            rewards.append(ep_reward)\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb164f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rando(env, continuous=False, total_episodes=10, render=False, timing=False, testing=False):\n",
    "    rewards = []\n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = env.action_space.sample() # np.random.randint(0,2,(4)).tolist()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward[0]\n",
    "\n",
    "            # For testing:\n",
    "            if testing:\n",
    "                time.sleep(0.5)\n",
    "                print(done)\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        if timing:\n",
    "            print(str(time.time() - before) + 's')\n",
    "            rewards.append(ep_reward)\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ma_gym:TrafficJunction4-v0')\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "# print(env.observation_space)\n",
    "obs, rew, done, _ = env.step(np.random.randint(0,2,(4)).tolist())\n",
    "print(obs[0].shape)\n",
    "print(rew)\n",
    "print(done)\n",
    "env.reset()\n",
    "env._max_steps = 80 # reduce from 100 for faster environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6731697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rando(env, continuous=False, total_episodes=10, render=True, timing=False, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca. best case reward\n",
    "-np.sum(np.arange(0, 0.01*14, 0.01)) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac46d97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1405\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1402\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1403\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m-> 1405\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m   1406\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1375\u001b[0m, in \u001b[0;36mmaybe_promote_tensors\u001b[1;34m(force_same_dtype, *tensors)\u001b[0m\n\u001b[0;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m promoted_tensors\n\u001b[1;32m-> 1375\u001b[0m result_type \u001b[39m=\u001b[39m np_dtypes\u001b[39m.\u001b[39;49m_result_type(\n\u001b[0;32m   1376\u001b[0m     \u001b[39m*\u001b[39;49m[_maybe_get_dtype(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m nest\u001b[39m.\u001b[39;49mflatten(tensors)])\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_promote_or_cast\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\numpy_ops\\np_dtypes.py:112\u001b[0m, in \u001b[0;36m_result_type\u001b[1;34m(*arrays_and_dtypes)\u001b[0m\n\u001b[0;32m    111\u001b[0m arrays_and_dtypes \u001b[39m=\u001b[39m [preprocess_float(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays_and_dtypes]\n\u001b[1;32m--> 112\u001b[0m dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mresult_type(\u001b[39m*\u001b[39;49marrays_and_dtypes)\n\u001b[0;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m dtypes\u001b[39m.\u001b[39mas_dtype(canonicalize_dtype(dtype))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mresult_type\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<tf.Variable 'layer_normalization_1/beta:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>' as a data type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ferdi\\Documents\\GitHub\\MADDPG\\MADDPG.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ag, rl, trl \u001b[39m=\u001b[39m run(env, \u001b[39mFalse\u001b[39;49;00m, total_trials\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, total_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         disc_actions_num\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m1453\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         buffer_capacity\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, num_agents\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, learn_step\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         std_dev\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, actor_lr\u001b[39m=\u001b[39;49m\u001b[39m0.0005\u001b[39;49m, critic_lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, min_clip\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, max_clip\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         gamma\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, tau\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, adam_eps\u001b[39m=\u001b[39;49m\u001b[39m1e-07\u001b[39;49m, amsgrad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, theta\u001b[39m=\u001b[39;49m\u001b[39m0.15\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         gamma_func\u001b[39m=\u001b[39;49mfixed, tau_func\u001b[39m=\u001b[39;49mfixed, critic_lr_func\u001b[39m=\u001b[39;49mfixed, actor_lr_func\u001b[39m=\u001b[39;49mfixed, std_dev_func\u001b[39m=\u001b[39;49mfixed, epsilon_func\u001b[39m=\u001b[39;49mfixed,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         reward_mod\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, start_steps\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, loss_func\u001b[39m=\u001b[39;49mlosses\u001b[39m.\u001b[39;49mMeanSquaredError(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         mean_number\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, solved\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m9\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, weights_directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mWeights/\u001b[39;49m\u001b[39m'\u001b[39;49m, plots_directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mGraphs/\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, total_time\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, use_gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_data\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Ferdi\\Documents\\GitHub\\MADDPG\\MADDPG.ipynb Cell 15\u001b[0m in \u001b[0;36mrun\u001b[1;34m(env, continuous, total_trials, total_episodes, disc_actions_num, seed, buffer_capacity, batch_size, num_agents, learn_step, std_dev, epsilon, actor_lr, critic_lr, min_clip, max_clip, gamma, tau, adam_eps, amsgrad, theta, gamma_func, tau_func, critic_lr_func, actor_lr_func, std_dev_func, epsilon_func, reward_mod, start_steps, loss_func, mean_number, solved, render, weights_directory, plots_directory, output, total_time, use_gpu, return_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m tf_prev_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(tf\u001b[39m.\u001b[39mconvert_to_tensor(prev_state), \u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m start_steps:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mpolicy(state\u001b[39m=\u001b[39;49mtf_prev_state, noise_object\u001b[39m=\u001b[39;49magent\u001b[39m.\u001b[39;49mou_noise, rng\u001b[39m=\u001b[39;49mrng)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     action \u001b[39m=\u001b[39m two_mini_random(num_agents)\n",
      "\u001b[1;32mc:\\Users\\Ferdi\\Documents\\GitHub\\MADDPG\\MADDPG.ipynb Cell 15\u001b[0m in \u001b[0;36mCoop_MADDPG.policy\u001b[1;34m(self, state, noise_object, use_noise, rng)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m             actions \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m             \u001b[39mfor\u001b[39;00m tf_obs \u001b[39min\u001b[39;00m state[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m                 actions\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor_model(tf\u001b[39m.\u001b[39;49mexpand_dims(tf_obs,\u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(actions)\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ferdi/Documents/GitHub/MADDPG/MADDPG.ipynb#X13sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    488\u001b[0m   layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\functional.py:458\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \n\u001b[0;32m    443\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[0;32m    459\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\functional.py:596\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    593\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 596\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    598\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py:324\u001b[0m, in \u001b[0;36mLayerNormalization.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    322\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mcast(scale, outputs\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    323\u001b[0m   \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     outputs \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39;49m tf\u001b[39m.\u001b[39;49mcast(offset, outputs\u001b[39m.\u001b[39;49mdtype)\n\u001b[0;32m    326\u001b[0m \u001b[39m# If some components of the shape got lost due to adjustments, fix that.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m outputs\u001b[39m.\u001b[39mset_shape(input_shape)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1417\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m   r_op \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(y, \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name)\n\u001b[1;32m-> 1417\u001b[0m   out \u001b[39m=\u001b[39m r_op(x)\n\u001b[0;32m   1418\u001b[0m   \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1419\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:1074\u001b[0m, in \u001b[0;36mVariable._OverloadOperator.<locals>._run_op\u001b[1;34m(a, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op\u001b[39m(a, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1073\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1074\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_oper(a\u001b[39m.\u001b[39mvalue(), \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1441\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.r_binary_op_wrapper\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\u001b[39mNone\u001b[39;00m, op_name, [x, y]) \u001b[39mas\u001b[39;00m name:\n\u001b[0;32m   1438\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m   y, x \u001b[39m=\u001b[39m maybe_promote_tensors(y, x, force_same_dtype\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1441\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1756\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1754\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39madd(x, y, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   1755\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1756\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49madd_v2(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:461\u001b[0m, in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    460\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    462\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mAddV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    464\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ag, rl, trl = run(env, False, total_trials=1, total_episodes=1000, \n",
    "        disc_actions_num=2, seed=1453,\n",
    "        buffer_capacity=50000, batch_size=128, num_agents=4, learn_step=8,\n",
    "        std_dev=0.3, epsilon=0.1, actor_lr=0.0005, critic_lr=0.001, min_clip=-2, max_clip=2,\n",
    "        gamma=0.99, tau=0.001, adam_eps=1e-07, amsgrad=False, theta=0.15,\n",
    "        gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed, std_dev_func=fixed, epsilon_func=fixed,\n",
    "        reward_mod=False, start_steps=0, loss_func=losses.MeanSquaredError(),\n",
    "        mean_number=20, solved=-9,\n",
    "        render=False, weights_directory='Weights/', plots_directory='Graphs/', \n",
    "        output=True, total_time=True, use_gpu=True, return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.done_buffer.tolist()\n",
    "with open(\"done_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6031cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.action_buffer.tolist()\n",
    "with open(\"action_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc527860",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.state_buffer.tolist()\n",
    "with open(\"state_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.next_state_buffer.tolist()\n",
    "with open(\"next_state_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfaa226",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag.reward_buffer.tolist()\n",
    "with open(\"reward_buffer.txt\", \"w\") as file1:\n",
    "    for l in a:\n",
    "        file1.writelines(str(l)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f46e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_world()\n",
    "# reset_world()\n",
    "# reward()\n",
    "# observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('simple.py')\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo.butterfly import pistonball_v6\n",
    "# parallel_env = pistonball_v6.parallel_env()\n",
    "# observations = parallel_env.reset()\n",
    "# env = pistonball_v6.parallel_env()\n",
    "# parallel_api_test(env, num_cycles=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b44c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
