{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764110b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distutils.log import error\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "def get_actor(num_states, num_actions, continuous, disc_actions_num):\n",
    "    \n",
    "    ### ACTOR NETWORK ###\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    # out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(64, activation=\"relu\")(out)\n",
    "    # out = layers.LayerNormalization(axis=1)(out)\n",
    "    \n",
    "    if continuous:\n",
    "        outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=initializers.RandomNormal(stddev=0.03))(out)\n",
    "    else:\n",
    "        outputs = layers.Dense(disc_actions_num, activation=\"softmax\", kernel_initializer=initializers.RandomNormal(stddev=0.03))(out)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num):\n",
    "    \n",
    "    ### CRITIC NETWORK ###\n",
    "    \n",
    "    state_input = layers.Input(shape=(num_states * num_agents))\n",
    "    state_out = layers.Dense(64, activation=\"relu\")(state_input)\n",
    "    \n",
    "    if continuous:\n",
    "        action_input = layers.Input(shape=(num_actions * num_agents))\n",
    "    else:\n",
    "        action_input = layers.Input(shape=(disc_actions_num * num_agents))\n",
    "    action_out = layers.Dense(64, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    out = layers.Dense(64, activation=\"relu\")(out)\n",
    "    out = layers.LayerNormalization(axis=1)(out)\n",
    "    outputs = layers.Dense(num_actions)(out)\n",
    "\n",
    "    return Model([state_input, action_input], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8df9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n",
    "        \n",
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.convert_to_tensor(np.zeros((64,4,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = a.numpy()\n",
    "batch[:,0,:] = np.ones((64,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d104979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coop_MADDPG:\n",
    "    def __init__(self, num_states, num_actions, num_agents, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, epsilon,\n",
    "            actor_lr, critic_lr, gamma, tau, min_clip, max_clip,\n",
    "            adam_eps, amsgrad, theta, disc_actions_num, loss_func):\n",
    "        \n",
    "        self.continuous = continuous\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # This is used to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_agents, num_states))\n",
    "        if self.continuous:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_agents, num_actions))\n",
    "        else:\n",
    "            self.action_buffer = np.zeros((self.buffer_capacity, num_agents, disc_actions_num))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, num_agents, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_agents, num_states))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, num_agents, 1), np.float32)\n",
    "        self.std_dev = std_dev # For continuous\n",
    "        self.epsilon = epsilon # Epsilon greedy for discrete\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.disc_actions_num = disc_actions_num\n",
    "        self.num_agents = num_agents\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        self.min_clip = min_clip\n",
    "        self.max_clip = max_clip\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1), theta=theta)\n",
    "        \n",
    "        self.actor_model = get_actor(num_states, num_actions, continuous, disc_actions_num)\n",
    "        self.critic_model = get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num)\n",
    "        self.target_actor = get_actor(num_states, num_actions, continuous, disc_actions_num)\n",
    "        self.target_critic = get_critic(num_states, num_agents, num_actions, continuous, disc_actions_num)\n",
    "        \n",
    "        self.actor_optimizer = optimizers.Adam(\n",
    "            learning_rate=actor_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_eps, amsgrad=amsgrad,\n",
    "        )\n",
    "        self.critic_optimizer = optimizers.Adam(\n",
    "            learning_rate=critic_lr, beta_1=0.9, beta_2=0.999, epsilon=adam_eps, amsgrad=amsgrad,\n",
    "        )\n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "    \n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        for agent in range(self.num_agents):\n",
    "            self.state_buffer[index][agent] = obs_tuple[0][agent]\n",
    "            self.action_buffer[index][agent] = obs_tuple[1][agent]\n",
    "            self.reward_buffer[index][agent] = obs_tuple[2][agent]\n",
    "            self.next_state_buffer[index][agent] = obs_tuple[3][agent]\n",
    "            self.done_buffer[index][agent] = obs_tuple[4][agent]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Calculation of loss and gradients\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch, loss_func):\n",
    "\n",
    "        # state_batch = state_batch.reshape(*state_batch.shape[:1], -1)\n",
    "        # action_batch = action_batch.reshape(*action_batch.shape[:1], -1)\n",
    "        # reward_batch = reward_batch.reshape(*reward_batch.shape[:1], -1)\n",
    "        # next_state_batch = next_state_batch.reshape(*next_state_batch.shape[:1], -1)\n",
    "        # done_batch = done_batch.reshape(*done_batch.shape[:1], -1)\n",
    "        #print(next_state_batch[:][0])\n",
    "        flat_next_state = next_state_batch.reshape(*next_state_batch.shape[:1], -1)\n",
    "        flat_state = state_batch.reshape(*state_batch.shape[:1], -1)\n",
    "        flat_action = action_batch.reshape(*action_batch.shape[:1], -1)\n",
    "\n",
    "        # calculate per agent loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = []\n",
    "            target_actions = np.zeros((self.batch_size, self.num_agents, self.disc_actions_num))\n",
    "            for agent in range(self.num_agents):\n",
    "                target_actions[:,agent,:] = self.target_actor(next_state_batch[:,agent,:], training=True).numpy()\n",
    "            target_actions = tf.convert_to_tensor(target_actions)\n",
    "            target_actions = target_actions.reshape(*target_actions.shape[:1], -1)\n",
    "            for agent in range(self.num_agents):\n",
    "                y.append(reward_batch[:,agent,:] + done_batch[:,agent,:] * self.gamma * self.target_critic([flat_next_state, target_actions], training=True))\n",
    "            critic_value = (self.critic_model([flat_state, flat_action], training=True))\n",
    "            critic_loss = loss_func(sum(y), critic_value)\n",
    "\n",
    "        # get the gradient for the critic\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        critic_gvd = zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        critic_capped_grad = [(tf.clip_by_value(grad, clip_value_min=self.min_clip, clip_value_max=self.max_clip), var) for grad, var in critic_gvd]\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(critic_capped_grad)\n",
    "\n",
    "        # calculate per agent\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = np.zeros((self.batch_size, self.num_agents, self.disc_actions_num))\n",
    "            for agent in range(self.num_agents):\n",
    "                actions[:,agent,:] = self.actor_model(state_batch[:,agent,:], training=True).numpy()\n",
    "            actions = tf.convert_to_tensor(target_actions)\n",
    "            actions = actions.reshape(*actions.shape[:1], -1)\n",
    "            critic_value = self.critic_model([flat_state, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "            # print(actor_loss)\n",
    "\n",
    "        # get the gradient for the critic\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "\n",
    "        # Gradient clipping\n",
    "        actor_gvd = zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        # print(*actor_gvd)\n",
    "        *agvd, = actor_gvd\n",
    "        actor_capped_grad = [(tf.clip_by_value(grad, clip_value_min=self.min_clip, clip_value_max=self.max_clip), var) for grad, var in actor_gvd]\n",
    "        \n",
    "        self.actor_optimizer.apply_gradients(actor_capped_grad)\n",
    "\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.done_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch, done_batch, self.loss_func)\n",
    "        \n",
    "    def policy(self, state, noise_object=0, use_noise=True, rng=np.random.default_rng()):\n",
    "        if use_noise:\n",
    "            if self.continuous:\n",
    "                # sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "                \n",
    "                # noise = noise_object()\n",
    "                \n",
    "                # sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "                # # We make sure action is within bounds\n",
    "                # legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                # return [np.squeeze(legal_action)][0]\n",
    "                raise ValueError('not implemented continuous')\n",
    "            else:\n",
    "                if (rng.random() < self.epsilon):\n",
    "                    # Return random array of actions\n",
    "                    actions = np.random.randint(0, self.disc_actions_num, (self.num_agents, self.num_actions))\n",
    "                    if actions.shape[1] == 1:\n",
    "                        # Must squeeze if each agent just have 1 action\n",
    "                        actions = actions.squeeze()\n",
    "                    return actions.tolist()\n",
    "                else:\n",
    "                    # Return output of the actor network for each of the observed states\n",
    "                    actions = []\n",
    "                    for tf_obs in state[0]:\n",
    "                        actions.append(np.argmax(self.actor_model(tf.expand_dims(tf_obs,0))))\n",
    "                    return actions\n",
    "        else:\n",
    "            if self.continuous:\n",
    "                # sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "                # legal_action = np.clip(sampled_actions, -1, 1)\n",
    "                # return [np.squeeze(legal_action)][0]\n",
    "                raise ValueError('not implemented continuous')\n",
    "            else:\n",
    "                # Return output of the actor network for each of the observed states\n",
    "                actions = []\n",
    "                for tf_obs in state[0]:\n",
    "                    actions.append(np.argmax(self.actor_model(tf.expand_dims(tf_obs,0))))\n",
    "                return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, continuous, total_trials=1, total_episodes=100, \n",
    "            buffer_capacity=50000, batch_size=64, \n",
    "            std_dev=0.3, epsilon=0.2, actor_lr=0.002, critic_lr=0.003, min_clip=-1, max_clip=1,\n",
    "            gamma=0.99, tau=0.005, adam_eps=1e-07, amsgrad=False, theta=0.15,\n",
    "            disc_actions_num=2, seed=1453,\n",
    "            render=False, directory='Weights/', output=True, total_time=True, use_gpu=True,\n",
    "            save_weights=True, return_rewards=False,\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed, std_dev_func=fixed, epsilon_func=fixed,\n",
    "            mean_number=20, solved=999,\n",
    "            reward_mod=False, start_steps=0, loss_func=losses.MeanAbsoluteError()):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # _ = env.seed(seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    num_agents = env.n_agents\n",
    "    num_states = env.observation_space[0].shape[0]\n",
    "    if continuous:\n",
    "        num_actions = env.action_space.shape[0]\n",
    "    else:\n",
    "        num_actions = 1\n",
    "    \n",
    "    if not use_gpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "    # Normalize action space according to https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
    "    # env.action_space = spaces.Box(low=-1, high=1, shape=(num_actions,), dtype='float32')\n",
    "    \n",
    "    ep_reward_list = []\n",
    "    avg_reward_list = []\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "        step = 0\n",
    "\n",
    "        # Add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Coop_MADDPG(num_states, num_actions, num_agents, continuous,\n",
    "            buffer_capacity, batch_size, std_dev, epsilon,\n",
    "            actor_lr, critic_lr, gamma, tau, min_clip, max_clip,\n",
    "            adam_eps, amsgrad, theta, disc_actions_num, loss_func)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            before = time.time()\n",
    "            \n",
    "            agent.gamma = gamma_func(agent.gamma, ep)\n",
    "            agent.tau = tau_func(agent.tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(agent.critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(agent.actor_lr, ep)\n",
    "            agent.std_dev = std_dev_func(agent.std_dev, ep)\n",
    "            agent.epsilon = epsilon_func(agent.epsilon, ep)\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = np.zeros(num_agents)\n",
    "            true_reward = np.zeros(num_agents)\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                if step >= start_steps:\n",
    "                    action = agent.policy(state=tf_prev_state, noise_object=agent.ou_noise, rng=rng)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                if continuous:\n",
    "                    # try:\n",
    "                    #     len(action)\n",
    "                    # except:\n",
    "                    #     action = [action]\n",
    "                    # state, reward, done, info = env.step(action)\n",
    "                    raise ValueError('not implemented continuous')\n",
    "                else:\n",
    "                    state, reward, done, info = env.step(action)\n",
    "                \n",
    "                true_reward = true_reward.__add__(reward)\n",
    "                \n",
    "                # Reward modification\n",
    "                if reward_mod:\n",
    "                    reward -= abs(state[0])\n",
    "\n",
    "                terminal_state = np.array(np.invert(done),dtype=np.float32)\n",
    "                \n",
    "                agent.record((prev_state, action, reward, state, terminal_state))\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                episodic_reward = episodic_reward.__add__(reward)\n",
    "\n",
    "                prev_state = state\n",
    "                \n",
    "                if all(done):\n",
    "                    break\n",
    "\n",
    "            episodic_reward = np.sum(episodic_reward)\n",
    "            true_reward = np.sum(true_reward)\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "            \n",
    "            if output:\n",
    "                if reward_mod:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * true AvgReward {:.2f} * Reward {:.2f} * True Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, true_avg_reward, sum(episodic_reward, true_reward, (time.time() - before), step)))\n",
    "                else:\n",
    "                    print(\"Ep {} * AvgReward {:.2f} * Reward {:.2f} * time {:.2f} * step {}\"\n",
    "                    .format(ep, avg_reward, episodic_reward, (time.time() - before), step))\n",
    "            \n",
    "            # Stop if avg is above 'solved'\n",
    "            if true_avg_reward >= solved:\n",
    "                break\n",
    "\n",
    "        # Save weights\n",
    "        now = datetime.datetime.now()\n",
    "        timestamp = \"{}.{}.{}.{}.{}.{}\".format(now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        save_name = \"{}_{}_{}\".format(env.spec.id, continuous, timestamp)\n",
    "        if save_weights:\n",
    "            try:\n",
    "                agent.actor_model.save_weights(directory + 'actor-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('actor save fail')\n",
    "            try:\n",
    "                agent.critic_model.save_weights(directory + 'critic-trial' + str(trial) + '_' + save_name + '.h5')\n",
    "            except:\n",
    "                print('critic save fail')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    try:\n",
    "        plt.savefig('Graphs/' + save_name + '.png')\n",
    "    except:\n",
    "        print('fig save fail')\n",
    "    plt.show()\n",
    "    \n",
    "    print('total time:', time.time() - start_time, 's')\n",
    "    \n",
    "    if return_rewards:\n",
    "        return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, actor_weights, continuous, total_episodes=10, render=False, timing=False):\n",
    "    rewards = []\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Coop_MADDPG()\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "            action = agent.policy(state=tf_prev_state, use_noise=False)\n",
    "\n",
    "            if continuous:\n",
    "                # try:\n",
    "                #     len(action)\n",
    "                # except:\n",
    "                #     action = [action]\n",
    "                state, reward, done, info = env.step(action)\n",
    "            else:\n",
    "                state, reward, done, info = env.step(np.argmax(action))\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        if timing:\n",
    "            print(str(time.time() - before) + 's')\n",
    "            rewards.append(ep_reward)\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb164f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(env, continuous=False, total_episodes=10, render=False, timing=False, testing=False):\n",
    "    rewards = []\n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = np.randint(0-1, 4)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward[0]\n",
    "\n",
    "            # For testing:\n",
    "            if testing:\n",
    "                time.sleep(0.5)\n",
    "                print(reward)\n",
    "            # ---\n",
    "\n",
    "            prev_state = state\n",
    "\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        if timing:\n",
    "            print(str(time.time() - before) + 's')\n",
    "            rewards.append(ep_reward)\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ma_gym:TrafficJunction4-v0')\n",
    "env.reset()\n",
    "print(env.action_space)\n",
    "# print(env.observation_space)\n",
    "obs, rew, done, _ = env.step(np.random.randint(0,2,(4)).tolist())\n",
    "print(obs[0].shape)\n",
    "print(rew)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac46d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(env, False, output=True, solved=-9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
